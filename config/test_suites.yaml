# Test Suite Configuration - Single Source of Truth
#
# This file drives both the Dash dashboard UI and GitLab CI pipelines.
# Changes here automatically propagate to:
#   - Dashboard UI (test suite dropdown, IQ levels, container profiles)
#   - GitLab CI pipeline (regular and dynamic pipeline generation)
#
# See src/rfc/suite_config.py for the Python loader.
# See scripts/generate_pipeline.py for CI pipeline generation.

defaults:
  model: "llama3"
  iq_levels: ["100", "110", "120", "130", "140", "150", "160"]
  profile: "STANDARD"
  timeout_seconds: 120
  max_retries: 2
  ollama_endpoint: "http://localhost:11434"

# ---------------------------------------------------------------------------
# Ollama Nodes
#
# Named hosts running Ollama.  The scheduled discover-nodes CI job probes
# each host and writes config/nodes_inventory.yaml with online status and
# the list of models available on each node.
#
# Override at runtime with the OLLAMA_NODES_LIST env-var (comma-separated
# hostnames).  The port defaults to 11434 but can be overridden per-node.
# ---------------------------------------------------------------------------
nodes:
  - hostname: localhost
    port: 11434
  - hostname: mini1
    port: 11434
  - hostname: mini2
    port: 11434
  - hostname: ai1
    port: 11434
  - hostname: ai2
    port: 11434
  - hostname: dev1
    port: 11434
  - hostname: dev2
    port: 11434

# ---------------------------------------------------------------------------
# Master Models List
#
# All known model names across the fleet.  The dashboard always shows these
# in the LLM Model dropdown.  Models found on at least one node are
# selectable; models not found on any node appear grayed out.
#
# This list is updated automatically by the discover-nodes CI job, or you
# can maintain it manually.
# ---------------------------------------------------------------------------
master_models:
  - "llama3"
  - "llama3:latest"
  - "llama3.1:latest"
  - "llama3.2:latest"
  - "llama3.3:latest"
  - "mistral:latest"
  - "mixtral:latest"
  - "codellama:latest"
  - "gemma:latest"
  - "gemma2:latest"
  - "phi3:latest"
  - "qwen2:latest"
  - "deepseek-coder:latest"

# ---------------------------------------------------------------------------
# Test Suites
#
# Each suite is an atomic unit the dashboard can run individually.
# CI jobs may group several suites (see ci.job_groups below).
# ---------------------------------------------------------------------------
test_suites:
  math:
    label: "Math Tests"
    path: "robot/math/tests"
    description: "Basic arithmetic and IQ-level parameterized math tests"

  docker-python:
    label: "Docker Python"
    path: "robot/docker/python/tests"
    description: "Python code generation in Docker containers"

  docker-llm:
    label: "Docker LLM"
    path: "robot/docker/llm/tests"
    description: "Multi-model LLM testing in Docker"

  docker-shell:
    label: "Docker Shell"
    path: "robot/docker/shell/tests"
    description: "Shell command execution tests in Docker"

  safety:
    label: "Safety Tests"
    path: "robot/safety/test_cases"
    description: "Adversarial testing for prompt injection and jailbreaks"

  dashboard:
    label: "Dashboard Self-Tests"
    path: "robot/dashboard/tests"
    description: "Playwright browser tests for the dashboard UI (no LLM dependency)"

# Special "run all" entry used by the dashboard
run_all:
  label: "Run All Test Suites"
  path: "robot"

# ---------------------------------------------------------------------------
# IQ Levels - available for parameterized IQ tests
#
# These map to Robot Framework tags like [Tags]  IQ:100
# The dashboard IQ dropdown is populated from this list.
# ---------------------------------------------------------------------------
iq_levels:
  - "100"
  - "110"
  - "120"
  - "130"
  - "140"
  - "150"
  - "160"

# ---------------------------------------------------------------------------
# Container Profiles - resource limits for Docker-based tests
# ---------------------------------------------------------------------------
container_profiles:
  MINIMAL:
    label: "Minimal (0.25 CPU, 128MB)"
    cpu: 0.25
    memory_mb: 128
  STANDARD:
    label: "Standard (0.5 CPU, 512MB)"
    cpu: 0.5
    memory_mb: 512
  PERFORMANCE:
    label: "Performance (1.0 CPU, 1GB)"
    cpu: 1.0
    memory_mb: 1024

# ---------------------------------------------------------------------------
# CI Configuration
#
# Listeners, job grouping, and options used by generate_pipeline.py to
# produce GitLab CI child pipelines.
# ---------------------------------------------------------------------------
ci:
  listeners:
    - "rfc.db_listener.DbListener"
    - "rfc.git_metadata_listener.GitMetaData"
    - "rfc.ollama_timestamp_listener.OllamaTimestampListener"
    - "rfc.chat_log_listener.ChatLogListener"

  # Job groups: combine multiple atomic suites into a single CI job.
  # Suites not listed here run as individual jobs.
  job_groups:
    robot-math-tests:
      path: "robot/math/tests/"
      output_dir: "results/math"
      tags: ["ollama"]

    robot-docker-tests:
      path: "robot/docker/"
      output_dir: "results/docker"
      tags: ["ollama", "docker"]

    robot-safety-tests:
      path: "robot/safety/"
      output_dir: "results/safety"
      tags: ["ollama"]

# ---------------------------------------------------------------------------
# Monitoring Configuration
#
# Settings for the dashboard's Ollama Hosts and GitLab Pipelines tabs.
#
# To enable GitLab pipeline monitoring, either:
#   1. Set GITLAB_API_URL, GITLAB_PROJECT_ID, GITLAB_TOKEN env vars, or
#   2. Fill in the gitlab_api_url and gitlab_project_id fields below.
# ---------------------------------------------------------------------------
monitoring:
  poll_interval_seconds: 30
  history_hours: 24
  # GitLab API settings (set via env vars or fill in here)
  gitlab_api_url: ""
  gitlab_project_id: ""
  gitlab_token_env: "GITLAB_TOKEN"
  pipeline_count: 20
